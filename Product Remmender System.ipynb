{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d54588-bdab-489e-bb5b-35294738ee86",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e49e8d12-e20f-409d-9a09-37256433bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint \n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fde729-d1d7-464a-ad78-9392d893c7f7",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a5b46",
   "metadata": {},
   "source": [
    "Firstly, we have to prepare the dataset and convert it the tensor style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3adb3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset():\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df_ = df[:100000]\n",
    "        self.users_df = pd.DataFrame\n",
    "\n",
    "    def feature_selection(self):\n",
    "        # The first 100,000 records were selected because the original dataset was too large to train\n",
    "        self.df_ = self.df_[self.df_['Quantity']>=1].dropna().reset_index(drop = True)\n",
    "\n",
    "        # Select  essential features \n",
    "        self.users_df = self.df_[['Customer ID', 'Description']]\n",
    "        help_ = self.users_df[['Description']].drop_duplicates()\n",
    "        help_['product_id'] = [i+1 for i in range(help_.shape[0])]  \n",
    "        self.users_df = self.users_df.merge(help_, on='Description'\n",
    "                            ).drop('Description', axis=1\n",
    "                            ).rename(columns = {'Description' : 'product_id', 'Customer ID': 'user_id'})\n",
    "        return self.users_df\n",
    "        \n",
    "    def create_tensor_dataset(self):\n",
    "        # Convert the dataframe to tensor format\n",
    "        self.users_df.user_id = self.users_df.user_id.apply(lambda x: str(int(x)))\n",
    "        self.users_df.product_id = self.users_df.product_id.apply(lambda x: str(x))\n",
    "\n",
    "        self.users_df = self.users_df.sample(frac=1).reset_index(drop=True)\n",
    "        self.products_df = self.users_df[['product_id']]   \n",
    "\n",
    "        self.users_dataset = tf.data.Dataset.from_tensor_slices(dict(self.users_df))\n",
    "        self.products_dataset = tf.data.Dataset.from_tensor_slices(dict(self.products_df))\n",
    "        return self.users_dataset, self.products_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555015ce-255f-42a5-b899-cc01f08039a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implement recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d5d033",
   "metadata": {},
   "source": [
    "After converting the dataset, it's time to implement the recommender system on the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d611f26b-33fa-43ba-bc39-83db15f3cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_manipulation:\n",
    "    \n",
    "    def __init__(self, users, products):\n",
    "        self.users = users\n",
    "        self.products = products\n",
    "        \n",
    "    # keep useful elements\n",
    "    def keep_useful_elements(self):\n",
    "        self.users = self.users.map(lambda x: {\n",
    "                         'product_id' : x['product_id'],\n",
    "                         'user_id' : x['user_id'],\n",
    "                    })\n",
    "        self.products = self.products.map(lambda x: x['product_id'])\n",
    "        return self.users, self.products \n",
    "    \n",
    "    # Train test split\n",
    "    def train_test_generator(self, train_range=80_000, all_range=100_000):\n",
    "        tf.random.set_seed(42)\n",
    "        shuffled = self.users.shuffle(all_range, seed=42, reshuffle_each_iteration=False)\n",
    "        train = shuffled.take(train_range)\n",
    "        test = shuffled.skip(train_range).take(all_range - train_range)\n",
    "        return train, test\n",
    "    \n",
    "    # Create a list of unique products and users\n",
    "    def pass_unique(self):\n",
    "        product_ids = self.products.batch(1_000)\n",
    "        user_ids = self.users.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "        unique_product_ids = np.unique(np.concatenate(list(product_ids)))\n",
    "        unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "        return unique_product_ids, unique_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dbc3f6f-7ad1-4ec1-8583-87d893bfc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelAndLoss:\n",
    "    \n",
    "    def __init__(self, unique_product_ids, unique_user_ids, products):\n",
    "        self.unique_product_ids = unique_product_ids\n",
    "        self.unique_user_ids = unique_user_ids\n",
    "        self.products = products\n",
    "\n",
    "    # Here, we're going to use Keras preprocessing layers to first convert user ids to integers, and then convert those\n",
    "    # to user embeddings via an Embedding layer.\n",
    "    def implement_model(self, embedding_dimension = 32):\n",
    "        user_model = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "              vocabulary=self.unique_user_ids, mask_token=None),\n",
    "          # Add an additional embedding to account for unknown tokens.\n",
    "          tf.keras.layers.Embedding(len(self.unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "        \n",
    "        # the candidate tower\n",
    "        self.product_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.StringLookup(\n",
    "          vocabulary=self.unique_product_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(self.unique_product_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "        return user_model, self.product_model\n",
    "    \n",
    "    def metrics_loss(self, batch_size = 128):\n",
    "        metrics = tfrs.metrics.FactorizedTopK(\n",
    "          candidates= self.products.batch(batch_size).map(self.product_model)\n",
    "        )\n",
    "        task = tfrs.tasks.Retrieval(\n",
    "            metrics=metrics)\n",
    "        return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "246ff110-713e-4332-b00b-79508af8c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class userProductModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, user_model, product_model):\n",
    "        super().__init__()\n",
    "        self.product_model: tf.keras.Model = product_model\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    # Now it's time to implement the full model\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        self.user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        # And pick out the product features and pass them into the product model,\n",
    "        # getting embeddings back.\n",
    "        self.positive_product_embeddings = self.product_model(features[\"product_id\"])\n",
    "\n",
    "        # The task computes loss and the metrics.\n",
    "        return self.task(self.user_embeddings, self.positive_product_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b93ff5e-8c90-444c-88e6-168ea3e01264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and evaluating\n",
    "class fitAndEvaluateModel:\n",
    "\n",
    "    # As the final stage, we create, compile, fit, and evaluate our model\n",
    "    \n",
    "    def __init__(self, user_model, product_model, train, test):\n",
    "        self.user_model = user_model\n",
    "        self.product_model = product_model\n",
    "        self.model = None\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = userProductModel(self.user_model, self.product_model) \n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "    \n",
    "    def fit_model(self):\n",
    "        cached_train = train.shuffle(200_000).batch(8192).cache()\n",
    "        self.cached_test = test.batch(4096).cache()\n",
    "        self.model.fit(cached_train, epochs=10)\n",
    "        return self.model\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        self.model.evaluate(self.cached_test, return_dict=True)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65afa3-2265-4d3f-91a8-dc7611b2e842",
   "metadata": {},
   "source": [
    "# Recommend items to users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cc12e",
   "metadata": {},
   "source": [
    "After fitting and evaluating the model it's time to make suggestions to users.\n",
    "In this example we considered user numnber \"15865\" as our sample, however, any user that exists in the dataset could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ee1089b-b884-4080-afd8-150cdf883929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, products, product_model):\n",
    "    # Create a model that takes in raw query features, and\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "    # recommends products out of the entire products dataset.\n",
    "    index.index_from_dataset(\n",
    "      tf.data.Dataset.zip((products.batch(100), products.batch(100).map(model.product_model)))\n",
    ")\n",
    "\n",
    "    _, ids = index(tf.constant(['15865']))\n",
    "    return ids, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee995adf-8712-405b-8158-f11548d05ac2",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17e34503-853f-4545-99c3-3919485ff7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 198s 22s/step - factorized_top_k/top_1_categorical_accuracy: 5.6490e-05 - factorized_top_k/top_5_categorical_accuracy: 5.6490e-05 - factorized_top_k/top_10_categorical_accuracy: 9.8857e-05 - factorized_top_k/top_50_categorical_accuracy: 3.8131e-04 - factorized_top_k/top_100_categorical_accuracy: 9.8857e-04 - loss: 67700.8672 - regularization_loss: 0.0000e+00 - total_loss: 67700.8672\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 197s 22s/step - factorized_top_k/top_1_categorical_accuracy: 0.0014 - factorized_top_k/top_5_categorical_accuracy: 0.0016 - factorized_top_k/top_10_categorical_accuracy: 0.0019 - factorized_top_k/top_50_categorical_accuracy: 0.0043 - factorized_top_k/top_100_categorical_accuracy: 0.0085 - loss: 64868.2703 - regularization_loss: 0.0000e+00 - total_loss: 64868.2703\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 203s 23s/step - factorized_top_k/top_1_categorical_accuracy: 0.0021 - factorized_top_k/top_5_categorical_accuracy: 0.0027 - factorized_top_k/top_10_categorical_accuracy: 0.0036 - factorized_top_k/top_50_categorical_accuracy: 0.0117 - factorized_top_k/top_100_categorical_accuracy: 0.0234 - loss: 61874.9016 - regularization_loss: 0.0000e+00 - total_loss: 61874.9016\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 208s 23s/step - factorized_top_k/top_1_categorical_accuracy: 0.0023 - factorized_top_k/top_5_categorical_accuracy: 0.0035 - factorized_top_k/top_10_categorical_accuracy: 0.0056 - factorized_top_k/top_50_categorical_accuracy: 0.0197 - factorized_top_k/top_100_categorical_accuracy: 0.0375 - loss: 59809.1672 - regularization_loss: 0.0000e+00 - total_loss: 59809.1672\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 209s 23s/step - factorized_top_k/top_1_categorical_accuracy: 0.0022 - factorized_top_k/top_5_categorical_accuracy: 0.0037 - factorized_top_k/top_10_categorical_accuracy: 0.0063 - factorized_top_k/top_50_categorical_accuracy: 0.0266 - factorized_top_k/top_100_categorical_accuracy: 0.0488 - loss: 58363.1789 - regularization_loss: 0.0000e+00 - total_loss: 58363.1789\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 212s 23s/step - factorized_top_k/top_1_categorical_accuracy: 0.0022 - factorized_top_k/top_5_categorical_accuracy: 0.0044 - factorized_top_k/top_10_categorical_accuracy: 0.0080 - factorized_top_k/top_50_categorical_accuracy: 0.0333 - factorized_top_k/top_100_categorical_accuracy: 0.0584 - loss: 57305.1980 - regularization_loss: 0.0000e+00 - total_loss: 57305.1980\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 208s 23s/step - factorized_top_k/top_1_categorical_accuracy: 0.0025 - factorized_top_k/top_5_categorical_accuracy: 0.0051 - factorized_top_k/top_10_categorical_accuracy: 0.0096 - factorized_top_k/top_50_categorical_accuracy: 0.0402 - factorized_top_k/top_100_categorical_accuracy: 0.0699 - loss: 56502.5578 - regularization_loss: 0.0000e+00 - total_loss: 56502.5578\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 208s 23s/step - factorized_top_k/top_1_categorical_accuracy: 0.0019 - factorized_top_k/top_5_categorical_accuracy: 0.0062 - factorized_top_k/top_10_categorical_accuracy: 0.0118 - factorized_top_k/top_50_categorical_accuracy: 0.0477 - factorized_top_k/top_100_categorical_accuracy: 0.0797 - loss: 55876.0746 - regularization_loss: 0.0000e+00 - total_loss: 55876.0746\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 233s 26s/step - factorized_top_k/top_1_categorical_accuracy: 0.0018 - factorized_top_k/top_5_categorical_accuracy: 0.0076 - factorized_top_k/top_10_categorical_accuracy: 0.0146 - factorized_top_k/top_50_categorical_accuracy: 0.0533 - factorized_top_k/top_100_categorical_accuracy: 0.0863 - loss: 55375.7285 - regularization_loss: 0.0000e+00 - total_loss: 55375.7285\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 253s 28s/step - factorized_top_k/top_1_categorical_accuracy: 0.0017 - factorized_top_k/top_5_categorical_accuracy: 0.0093 - factorized_top_k/top_10_categorical_accuracy: 0.0169 - factorized_top_k/top_50_categorical_accuracy: 0.0572 - factorized_top_k/top_100_categorical_accuracy: 0.0926 - loss: 54968.4781 - regularization_loss: 0.0000e+00 - total_loss: 54968.4781\n"
     ]
    }
   ],
   "source": [
    "path = './Datasets/online_retail_II.xlsx'\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "dataset = PrepareDataset(df)\n",
    "users_df = dataset.feature_selection()\n",
    "users, products = dataset.create_tensor_dataset()\n",
    "\n",
    "data = data_manipulation(users, products)\n",
    "users, products = data.keep_useful_elements()\n",
    "train, test = data.train_test_generator()\n",
    "unique_product_ids, unique_user_ids = data.pass_unique()\n",
    "\n",
    "pre_model = modelAndLoss(unique_product_ids, unique_user_ids, products)\n",
    "user_model, product_model = pre_model.implement_model()\n",
    "task = pre_model.metrics_loss()\n",
    "product_model\n",
    "\n",
    "model = fitAndEvaluateModel(user_model, product_model, train, test)\n",
    "model.create_model()\n",
    "model.compile_model()\n",
    "model_ = model.fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f2567",
   "metadata": {},
   "source": [
    "# Recommend products to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45bbec1b-e6b1-4bcc-9138-d1cc60c7580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 15865: [b'2741' b'2740' b'1196']\n"
     ]
    }
   ],
   "source": [
    "titles, index = make_predictions(model_, products, product_model)\n",
    "print(f\"Recommendations for user 15865: {titles[0, :3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
